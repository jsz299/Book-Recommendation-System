{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-02T17:18:15.036170Z",
     "start_time": "2024-10-02T17:18:13.707576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books Data:\n",
      "         ISBN                                         Book-Title  \\\n",
      "0  0195153448                                Classical Mythology   \n",
      "1  0002005018                                       Clara Callan   \n",
      "2  0060973129                               Decision in Normandy   \n",
      "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
      "4  0393045218                             The Mummies of Urumchi   \n",
      "\n",
      "            Book-Author Year-Of-Publication                   Publisher  \\\n",
      "0    Mark P. O. Morford                2002     Oxford University Press   \n",
      "1  Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
      "2          Carlo D'Este                1991             HarperPerennial   \n",
      "3      Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
      "4       E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
      "\n",
      "                                         Image-URL-S  \\\n",
      "0  http://images.amazon.com/images/P/0195153448.0...   \n",
      "1  http://images.amazon.com/images/P/0002005018.0...   \n",
      "2  http://images.amazon.com/images/P/0060973129.0...   \n",
      "3  http://images.amazon.com/images/P/0374157065.0...   \n",
      "4  http://images.amazon.com/images/P/0393045218.0...   \n",
      "\n",
      "                                         Image-URL-M  \\\n",
      "0  http://images.amazon.com/images/P/0195153448.0...   \n",
      "1  http://images.amazon.com/images/P/0002005018.0...   \n",
      "2  http://images.amazon.com/images/P/0060973129.0...   \n",
      "3  http://images.amazon.com/images/P/0374157065.0...   \n",
      "4  http://images.amazon.com/images/P/0393045218.0...   \n",
      "\n",
      "                                         Image-URL-L  \n",
      "0  http://images.amazon.com/images/P/0195153448.0...  \n",
      "1  http://images.amazon.com/images/P/0002005018.0...  \n",
      "2  http://images.amazon.com/images/P/0060973129.0...  \n",
      "3  http://images.amazon.com/images/P/0374157065.0...  \n",
      "4  http://images.amazon.com/images/P/0393045218.0...  \n",
      "\n",
      "Users Data:\n",
      "   User-ID                            Location   Age\n",
      "0        1                  nyc, new york, usa   NaN\n",
      "1        2           stockton, california, usa  18.0\n",
      "2        3     moscow, yukon territory, russia   NaN\n",
      "3        4           porto, v.n.gaia, portugal  17.0\n",
      "4        5  farnborough, hants, united kingdom   NaN\n",
      "\n",
      "Ratings Data:\n",
      "   User-ID        ISBN  Book-Rating\n",
      "0   276725  034545104X            0\n",
      "1   276726  0155061224            5\n",
      "2   276727  0446520802            0\n",
      "3   276729  052165615X            3\n",
      "4   276729  0521795028            6\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For collaborative filtering\n",
    "from surprise import SVD, Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import cross_validate, train_test_split\n",
    "\n",
    "# For neural network model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the datasets\n",
    "books = pd.read_csv('data/Books.csv', sep=',', error_bad_lines=False, encoding='latin-1')\n",
    "users = pd.read_csv('data/Users.csv', sep=',', error_bad_lines=False, encoding='latin-1')\n",
    "ratings = pd.read_csv('data/Ratings.csv', sep=',', error_bad_lines=False, encoding='latin-1')\n",
    "\n",
    "# Preview the data\n",
    "print('Books Data:')\n",
    "print(books.head())\n",
    "print('\\nUsers Data:')\n",
    "print(users.head())\n",
    "print('\\nRatings Data:')\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "\n",
    "# Rename columns for ease of use\n",
    "books.columns = ['ISBN', 'BookTitle', 'BookAuthor', 'YearOfPublication', 'Publisher', 'ImageURLS', 'ImageURLM', 'ImageURLL']\n",
    "users.columns = ['UserID', 'Location', 'Age']\n",
    "ratings.columns = ['UserID', 'ISBN', 'BookRating']\n",
    "\n",
    "# Handle missing values\n",
    "books['YearOfPublication'] = pd.to_numeric(books['YearOfPublication'], errors='coerce')\n",
    "books['YearOfPublication'].fillna(int(books['YearOfPublication'].mean()), inplace=True)\n",
    "books['YearOfPublication'] = books['YearOfPublication'].astype(int)\n",
    "\n",
    "users['Age'].fillna(users['Age'].mean(), inplace=True)\n",
    "users['Age'] = users['Age'].astype(int)\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "\n",
    "# Distribution of Book Ratings\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(ratings['BookRating'])\n",
    "plt.title('Distribution of Book Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Number of Ratings per Book\n",
    "ratings_per_book = ratings.groupby('ISBN')['BookRating'].count().reset_index().rename(columns={'BookRating': 'RatingCount'})\n",
    "print('\\nTop 5 Books with Most Ratings:')\n",
    "print(ratings_per_book.sort_values('RatingCount', ascending=False).head())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-10-02T17:20:14.029668Z"
    }
   },
   "id": "bb4541199814bcb0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['UserID', 'BookRating'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Collaborative Filtering using Matrix Factorization (SVD)\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Prepare data for Surprise library\u001B[39;00m\n\u001B[1;32m      4\u001B[0m reader \u001B[38;5;241m=\u001B[39m Reader(rating_scale\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m10\u001B[39m))\n\u001B[0;32m----> 5\u001B[0m data \u001B[38;5;241m=\u001B[39m Dataset\u001B[38;5;241m.\u001B[39mload_from_df(\u001B[43mratings\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mUserID\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mISBN\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mBookRating\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m, reader)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Build and evaluate the SVD model\u001B[39;00m\n\u001B[1;32m      8\u001B[0m trainset, testset \u001B[38;5;241m=\u001B[39m train_test_split(data, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m)\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/pandas/core/frame.py:3464\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3462\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[1;32m   3463\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[0;32m-> 3464\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_listlike_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   3466\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[1;32m   3467\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/pandas/core/indexing.py:1314\u001B[0m, in \u001B[0;36m_LocIndexer._get_listlike_indexer\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1311\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1312\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m ax\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[0;32m-> 1314\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_read_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m needs_i8_conversion(ax\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m   1317\u001B[0m     ax, (IntervalIndex, CategoricalIndex)\n\u001B[1;32m   1318\u001B[0m ):\n\u001B[1;32m   1319\u001B[0m     \u001B[38;5;66;03m# For CategoricalIndex take instead of reindex to preserve dtype.\u001B[39;00m\n\u001B[1;32m   1320\u001B[0m     \u001B[38;5;66;03m#  For IntervalIndex this is to map integers to the Intervals they match to.\u001B[39;00m\n\u001B[1;32m   1321\u001B[0m     keyarr \u001B[38;5;241m=\u001B[39m ax\u001B[38;5;241m.\u001B[39mtake(indexer)\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/pandas/core/indexing.py:1377\u001B[0m, in \u001B[0;36m_LocIndexer._validate_read_indexer\u001B[0;34m(self, key, indexer, axis)\u001B[0m\n\u001B[1;32m   1374\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1376\u001B[0m not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[0;32m-> 1377\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"['UserID', 'BookRating'] not in index\""
     ]
    }
   ],
   "source": [
    "# Collaborative Filtering using Matrix Factorization (SVD)\n",
    "\n",
    "# Prepare data for Surprise library\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "data = Dataset.load_from_df(ratings[['UserID', 'ISBN', 'BookRating']], reader)\n",
    "\n",
    "# Build and evaluate the SVD model\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "svd_model = SVD()\n",
    "svd_model.fit(trainset)\n",
    "predictions = svd_model.test(testset)\n",
    "\n",
    "# Evaluate the model\n",
    "from surprise import accuracy\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f'\\nSVD Model RMSE: {rmse}')\n",
    "\n",
    "# Collaborative Filtering using KNN\n",
    "knn_model = KNNBasic()\n",
    "knn_model.fit(trainset)\n",
    "predictions_knn = knn_model.test(testset)\n",
    "rmse_knn = accuracy.rmse(predictions_knn)\n",
    "print(f'KNN Model RMSE: {rmse_knn}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T17:19:35.951394Z",
     "start_time": "2024-10-02T17:19:35.450527Z"
    }
   },
   "id": "d6f331fd9b3b3290",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Neural Collaborative Filtering using TensorFlow/Keras\n",
    "\n",
    "# Prepare data for neural network model\n",
    "user_ids = ratings['UserID'].unique().tolist()\n",
    "user_id_to_index = {x: i for i, x in enumerate(user_ids)}\n",
    "ratings['user_index'] = ratings['UserID'].map(user_id_to_index)\n",
    "\n",
    "book_ids = ratings['ISBN'].unique().tolist()\n",
    "book_id_to_index = {x: i for i, x in enumerate(book_ids)}\n",
    "ratings['book_index'] = ratings['ISBN'].map(book_id_to_index)\n",
    "\n",
    "# Define model architecture\n",
    "num_users = ratings['user_index'].nunique()\n",
    "num_books = ratings['book_index'].nunique()\n",
    "\n",
    "# Input layers\n",
    "user_input = Input(shape=(1,))\n",
    "book_input = Input(shape=(1,))\n",
    "\n",
    "# Embedding layers\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=50, input_length=1)(user_input)\n",
    "book_embedding = Embedding(input_dim=num_books, output_dim=50, input_length=1)(book_input)\n",
    "\n",
    "# Flatten layers\n",
    "user_vec = Flatten()(user_embedding)\n",
    "book_vec = Flatten()(book_embedding)\n",
    "\n",
    "# Concatenate user and book vectors\n",
    "concat = Concatenate()([user_vec, book_vec])\n",
    "\n",
    "# Add dense layers\n",
    "dense = Dense(128, activation='relu')(concat)\n",
    "dense = Dense(64, activation='relu')(dense)\n",
    "output = Dense(1)(dense)\n",
    "\n",
    "# Build and compile the model\n",
    "model = Model(inputs=[user_input, book_input], outputs=output)\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dbfe129e583f5cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Prepare training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = ratings[['user_index', 'book_index']]\n",
    "y = ratings['BookRating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([X_train['user_index'], X_train['book_index']], y_train,\n",
    "                    batch_size=256, epochs=5, verbose=1,\n",
    "                    validation_data=([X_test['user_index'], X_test['book_index']], y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict([X_test['user_index'], X_test['book_index']])\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'\\nNeural Network Model RMSE: {rmse_nn}')\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d108230e7456acd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def recommend_books(user_id, num_recommendations=5):\n",
    "    # Check if user_id exists\n",
    "    if user_id not in user_id_to_index:\n",
    "        print(\"User ID not found.\")\n",
    "        return\n",
    "    user_idx = user_id_to_index[user_id]\n",
    "    # Books not yet rated by the user\n",
    "    books_not_rated = ratings[~ratings['ISBN'].isin(\n",
    "        ratings[ratings['UserID'] == user_id]['ISBN'].tolist()\n",
    "    )]['ISBN'].unique()\n",
    "    books_not_rated_idx = [book_id_to_index[x] for x in books_not_rated]\n",
    "    user_idx_array = np.array([user_idx for _ in range(len(books_not_rated_idx))])\n",
    "    predictions = model.predict([user_idx_array, np.array(books_not_rated_idx)])\n",
    "    top_indices = predictions.flatten().argsort()[-num_recommendations:][::-1]\n",
    "    recommended_isbns = [books_not_rated[i] for i in top_indices]\n",
    "    recommended_books = books[books['ISBN'].isin(recommended_isbns)]\n",
    "    return recommended_books[['BookTitle', 'BookAuthor', 'YearOfPublication']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffdf21e28697a4c6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Example: Recommend books for a user\n",
    "user_to_recommend = 276725  # Replace with a valid UserID from your dataset\n",
    "recommended_books = recommend_books(user_to_recommend)\n",
    "print(f'\\nRecommended Books for User {user_to_recommend}:')\n",
    "print(recommended_books)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b74f19aa216108e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
